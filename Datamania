# ================================================
# Pipeline Completo para Predicci√≥n de Huevos de Dengue
# (VERSI√ìN SIN TRANSFORMACI√ìN LOGAR√çTMICA)
#
# v2.2 - Corregido para ejecuci√≥n local (limpieza de CSV y errores de √≠ndice)
# ================================================
#
# Este script combina todas las celdas del notebook
# en el orden correcto de ejecuci√≥n, pero entrena
# el modelo directamente sobre la variable 'eggs' original.
#
# VERSI√ìN LOCAL: Eliminada la dependencia 'google.colab'
# ================================================

# =======================
#  1. Importar librer√≠as
# =======================
print("--- 1. Importando librer√≠as ---")
import os # Es buena idea importar 'os' fuera del 'try' si lo usas en el 'except'
import warnings

try:
    # --- 1. Intenta importar todas las librer√≠as ---
    import pandas as pd
    import numpy as np
    import matplotlib.pyplot as plt
    from scipy.spatial import cKDTree
    from sklearn.model_selection import train_test_split, GridSearchCV
    from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
    from xgboost import XGBRegressor
    
    # Librer√≠as Geoespaciales
    import geopandas as gpd 
    from pysal.lib import weights
    from pysal.explore import esda
    from splot.esda import lisa_cluster
    
    print("‚úÖ Librer√≠as cargadas exitosamente.")

except ImportError as e:
    # --- 2. Si CUALQUIER importaci√≥n falla, se ejecuta este bloque ---
    print(f"‚ö†Ô∏è Error al importar la librer√≠a: {e}")
    print("üîß Intentando instalar las dependencias faltantes...")
    
    # Lista de todas las librer√≠as a instalar
    # (Aseg√∫rate de incluir todas las que est√°n en el 'try')
    install_command = 'pip install pandas numpy matplotlib scipy scikit-learn xgboost geopandas pysal splot --quiet'
    
    try:
        os.system(install_command)
        print("\n‚úÖ Instalaci√≥n completada.")
        print("üëâ Por favor, RE-EJECUTA el script para cargar las nuevas librer√≠as.")
        exit() # Salir para que el kernel recargue las librer√≠as instaladas
        
    except Exception as install_e:
        print(f"‚ùå Fall√≥ la instalaci√≥n autom√°tica: {install_e}")
        print("üëâ Intenta instalar manualmente el paquete mencionado en el error.")
        exit()
print("Iniciando an√°lisis...")

# Ignorar advertencias futuras
warnings.simplefilter(action='ignore', category=FutureWarning)

# =============================
# 2. Definir archivos locales
# =============================
print("\n--- 2. Definiendo nombres de archivos locales ---")
# --- Aseg√∫rate de que estos archivos est√©n en la misma carpeta que el script ---
covar_filename = 'covariables_con_decimales.xlsx'
eggs_filename = 'eggs_data.csv'

# *** NUEVO: Define el nombre de tu archivo Shapefile ***
# (Aseg√∫rate de que este archivo, y sus .dbf, .prj, .shx,
#  est√©n en el mismo directorio que el script)
shapefile_filename = '2024_31050_M23102025_1415.shp'
# ---------------------------------------------------------------------


# ============================
# 3. Cargar y limpiar nombres
# ============================
print("\n--- 3. Cargando y limpiando datos ---")
try:
    # --- Carga directa de archivos locales ---
    covar_df = pd.read_excel(covar_filename)
    
    # *** CORRECCI√ìN 1: Se a√±ade 'header=1' para saltar la l√≠nea mala ("<<<<<<< HEAD") ***
    eggs_df = pd.read_csv(eggs_filename, header=1)
    # ----------------------------------------

    covar_df.columns = [c.strip().lower() for c in covar_df.columns]
    eggs_df.columns = [c.strip().lower() for c in eggs_df.columns]

    print("Covariables:", covar_df.shape)
    print("Eggs data:", eggs_df.shape)

except FileNotFoundError:
    print(f"‚ùå Error: No se encontraron los archivos '{covar_filename}' o '{eggs_filename}'.")
    print("üëâ Aseg√∫rate de que los archivos est√°n en la misma carpeta que este script.")
    exit()
except Exception as e:
    print(f"‚ùå Error al cargar los archivos: {e}")
    exit()

# =========================================================
# 4. Combinar bases por coordenadas (uni√≥n por proximidad)
# =========================================================
print("\n--- 4. Combinando bases por coordenadas ---")
# covar_df.columns = [c.strip().lower() for c in covar_df.columns] # Ya se hizo en Secci√≥n 3
# eggs_df.columns  = [c.strip().lower() for c in eggs_df.columns] # Ya se hizo en Secci√≥n 3

# Esta l√≠nea es correcta, 'covariables' S√ç tiene 'longitud_dec' y 'latitud_dec'
covar_df = covar_df.rename(columns={"longitud_dec":"x", "latitud_dec":"y"}, errors="ignore")

# *** CORRECCI√ìN 2: 'eggs_data.csv' YA tiene 'x' e 'y', as√≠ que esta l√≠nea se comenta ***
# eggs_df  = eggs_df.rename(columns={"longitud_dec":"x", "latitud_dec":"y"}, errors="ignore")


for col in ["x","y"]:
    if col in covar_df: covar_df[col] = pd.to_numeric(covar_df[col], errors="coerce")
    if col in eggs_df:  eggs_df[col]  = pd.to_numeric(eggs_df[col],  errors="coerce")

print("Imputando coordenadas (x, y) faltantes con la mediana...")
for df_temp, nombre_df in [(covar_df, "covariables"), (eggs_df, "eggs_data")]:
    for col in ["x", "y"]:
        if col in df_temp and df_temp[col].isna().any():
            mediana = df_temp[col].median()
            if pd.isna(mediana):
                print(f"‚ö† Toda la columna '{col}' en {nombre_df} es NaN. Rellenando con 0.")
                mediana = 0
            n_nans = df_temp[col].isna().sum()
            print(f"    Imputando {n_nans} NaNs en '{col}' de {nombre_df} con {mediana:.4f}")
            df_temp[col] = df_temp[col].fillna(mediana)

# Esta l√≠nea (la que daba el primer KeyError) ahora funcionar√°
covar_df = covar_df.dropna(subset=["x","y"])
eggs_df = eggs_df.dropna(subset=["x","y"])
print(f"    Tama√±os post-imputaci√≥n: covar={len(covar_df)}, eggs={len(eggs_df)}")

if covar_df.empty or eggs_df.empty:
    raise ValueError("Uno de los dataframes (covar o eggs) est√° vac√≠o despu√©s de limpiar coordenadas.")

print(f"    Columnas en covar_df: {list(covar_df.columns)}")
print(f"    Columnas en eggs_df: {list(eggs_df.columns)}")
tree = cKDTree(covar_df[["x","y"]].to_numpy())
dist, idx = tree.query(eggs_df[["x","y"]].to_numpy(), k=1)

tol = 0.08
mask = dist <= tol
eggs_matched  = eggs_df.loc[mask].reset_index(drop=True)
covar_matched = covar_df.iloc[idx[mask]].reset_index(drop=True)

df = pd.concat([eggs_matched, covar_matched.drop(columns=["x","y"], errors="ignore")], axis=1)
print(f"Filas unidas por proximidad: {len(df)} de {len(eggs_df)}")


# ===============================================
# 5. Limpieza de predictoras (X) - make numeric y OHE
# ===============================================
print("\n--- 5. Limpiando variables predictoras (X) ---")
target = "eggs"

# *** CORRECCI√ìN 3: Limpiar filas corruptas (de conflictos de Git) ***
# Forzar la conversi√≥n de la columna 'target' a num√©rico.
# 'errors='coerce'' convertir√° cualquier texto (como 'eggs' o '=======') en NaN.
print(f"Coercionando columna target '{target}' a num√©rico para limpiar basura...")
original_len = len(df)
df[target] = pd.to_numeric(df[target], errors='coerce')

# Ahora, eliminamos las filas donde el target se convirti√≥ en NaN (filas corruptas).
df = df.dropna(subset=[target])
new_len = len(df)
if original_len > new_len:
    print(f"    Se eliminaron {original_len - new_len} filas con valores no num√©ricos en '{target}'.")
# *** FIN DE LA CORRECCI√ìN 3 ***


numerical_features = [
    "vph_pisoti","vph_c_elec","vph_s_elec","vph_aeasp",
    "vph_aguafv","vph_tinaco","vph_letr",
    "vph_drenaj","vph_nodren","vph_c_serv","vph_dsadma",
    "vph_lavad","graproes","precip_promedio","evap","tem_p_semana"
]
categorical_features = ["week"]

df_cols_lower = [str(c).lower() for c in df.columns]
existing_numerical = [col for col in numerical_features if col in df_cols_lower]
existing_categorical = [col for col in categorical_features if col in df_cols_lower]

if not existing_numerical or not existing_categorical:
    print("‚ö† Advertencia: Algunas columnas num√©ricas o categ√≥ricas no se encontraron en 'df'.")

X_num = df[existing_numerical].copy()
X_cat = df[existing_categorical].copy()

# Esta l√≠nea (la que daba el segundo ValueError) ahora funcionar√°
y_original = df[target].astype(float) # Esta es ahora nuestra Y principal

print("Procesando variables num√©ricas...")
bad_tokens = {"*","NA","N/A","na","-","‚Äî","", "sd", "s/d"}
X_num = X_num.map(lambda v: np.nan if (isinstance(v, str) and v.strip() in bad_tokens) else v)
for c in X_num.columns:
    X_num[c] = pd.to_numeric(X_num[c], errors="coerce")
nan_report = X_num.isna().sum().sort_values(ascending=False)
print("    NaNs por columna (num√©ricas) tras coerci√≥n:\n", nan_report[nan_report > 0])
all_nan_cols = [c for c in X_num.columns if X_num[c].isna().all()]
if all_nan_cols:
    print("‚ö† Columnas num√©ricas removidas por quedar totalmente en NaN:", all_nan_cols)
    X_num = X_num.drop(columns=all_nan_cols)
if not X_num.empty:
    medianas = X_num.median(numeric_only=True)
    X_num = X_num.fillna(medianas)
    print("    Imputaci√≥n con mediana completa (num√©ricas).")
else:
    print("‚ö† No quedaron variables num√©ricas despu√©s de la limpieza.")

print("\nAplicando One-Hot Encoding a 'week'...")
if not X_cat.empty:
    if X_cat['week'].isna().any():
        if not X_cat['week'].mode().empty:
            moda_week = X_cat['week'].mode()[0]
            print(f"    Imputando {X_cat['week'].isna().sum()} NaNs en 'week' con la moda: {moda_week}")
            X_cat['week'] = X_cat['week'].fillna(moda_week)
        else:
            print("‚ö† Columna 'week' es toda NaN, rellenando con 'desconocido'")
            X_cat['week'] = X_cat['week'].fillna('desconocido')
    X_cat['week'] = X_cat['week'].astype(str)
    X_ohe = pd.get_dummies(X_cat, columns=['week'], prefix='week', drop_first=False)
    print(f"    Se crearon {X_ohe.shape[1]} columnas OHE para 'week'.")
else:
    print("‚ö† No se encontr√≥ la columna 'week' para OHE.")
    X_ohe = pd.DataFrame()

# *** CORRECCI√ìN 4: Quitar reset_index para mantener el √≠ndice original ***
X = pd.concat([X_num, X_ohe], axis=1)
print("\nForma de X (antes de features espaciales):", X.shape)


# ===============================================
# 6. An√°lisis Exploratorio Espacial (ESDA - LISA)
# ===============================================
print("\n--- 6. Iniciando An√°lisis Exploratorio Espacial (LISA) ---")

print("Librer√≠as espaciales (geopandas, pysal, esda, splot) ya cargadas.")

print("Preparando datos espaciales...")
# Se usa 'df' (ya limpio) para crear el GeoDataFrame
gdf = gpd.GeoDataFrame(
    df, geometry=gpd.points_from_xy(df.x, df.y), crs="EPSG:4326"
)
y_esda = y_original # Usamos la Y original para Moran y LISA
gdf['eggs'] = y_esda # A√±adir al GeoDataFrame para LISA

print("Calculando matriz de pesos espaciales (k=8 vecinos)...")
try:
    wq = weights.KNN.from_dataframe(gdf, k=8)
    wq.transform = 'r'
except Exception as e:
    print(f"‚ùå Error al crear la matriz de pesos: {e}")
    exit()

print("\nCalculando Moran's I Global (sobre 'eggs' original)...")
try:
    mi_global = esda.Moran(y_esda, wq) # *** Usar y_esda (original) ***
    print(f"    Estad√≠stico Moran's I Global: {mi_global.I:.4f}")
    print(f"    P-valor (Global): {mi_global.p_sim:.4f}")
    if mi_global.p_sim < 0.05:
        print("    Interpretaci√≥n: ¬°Autocorrelaci√≥n Global SIGNIFICATIVA!")
    else:
        print("    Interpretaci√≥n: Autocorrelaci√≥n Global NO significativa.")
except Exception as e:
    print(f"‚ùå Error al calcular Moran Global: {e}")

print("\nCalculando I de Moran Local (LISA) (sobre 'eggs' original)...")
try:
    lisa = esda.Moran_Local(y_esda, wq) # *** Usar y_esda (original) ***
except Exception as e:
    print(f"‚ùå Error al calcular LISA: {e}")
    exit()

print("Generando Gr√°fico: Mapa de Clusters LISA (sobre 'eggs' original)...")
try:
    fig, ax = plt.subplots(figsize=(12, 10))
    lisa_cluster(lisa, gdf, p=0.05, ax=ax, legend=True)
    ax.set_title("Mapa de Clusters (LISA) para eggs (original)") # *** T√≠tulo cambiado ***
    ax.set_xlabel("Longitud (x)")
    ax.set_ylabel("Latitud (y)")
    plt.show() # Muestra el gr√°fico en una ventana
except Exception as e:
    print(f"‚ùå Error al generar el mapa LISA: {e}")


# ================================================
# 7. Ingenier√≠a de Variables Espaciales
# ================================================
print("\n--- 7. Creando variables espaciales ---")

print("Calculando 'Spatial Lag' (sobre 'eggs' original)...")
y_lag = weights.lag_spatial(wq, y_original)
spatial_lag_df = pd.DataFrame(y_lag, columns=['y_lag'], index=gdf.index)

print("Creando variable categ√≥rica de clusters LISA...")
lisa_quadrant_df = pd.DataFrame(lisa.q, columns=['lisa_q'], index=gdf.index)
lisa_quadrant_df['lisa_q'] = lisa_quadrant_df['lisa_q'].map({
    1: 'HH_HotSpot', 2: 'LH_Outlier', 3: 'LL_ColdSpot',
    4: 'HL_Outlier', 0: 'NS_NoSig'
})
lisa_ohe = pd.get_dummies(lisa_quadrant_df, columns=['lisa_q'], drop_first=True)

print("Uniendo 'X' original con las nuevas variables espaciales...")
# *** CORRECCI√ìN 5: Asignar X directamente (sin reset_index) ***
X_original_fe = X # Usar X de la secci√≥n 5 (con el √≠ndice correcto)
X_spatial = pd.concat([X_original_fe, spatial_lag_df, lisa_ohe], axis=1)
X_spatial = X_spatial.fillna(0)

print("\n‚úÖ Se cre√≥ la nueva matriz 'X_spatial'.")
print(f"    Forma 'X' original: {X_original_fe.shape}")
print(f"    Forma 'X_spatial' nueva: {X_spatial.shape}")


# ================================================
# 8. Divisi√≥n Train / Test (con X_spatial y SIN Log)
# ================================================
print("\n--- 8. Dividiendo datos para entrenamiento y prueba (SIN Log) ---")

print("Dividiendo 'X_spatial' y 'y_original'...")
# Esta l√≠nea (la que daba el tercer ValueError) ahora funcionar√°
X_train, X_test, y_train, y_test = train_test_split(
    X_spatial, y_original, test_size=0.2, random_state=42
)

# Ya no necesitamos y_train_orig, y_test_orig

print("\n‚úÖ ¬°Divisi√≥n completada!")
print(f"    X_train: {X_train.shape}, y_train: {y_train.shape}")
print(f"    X_test:  {X_test.shape}, y_test:  {y_test.shape}")


# ================================================
# 9. Entrenar XGBoost Regressor (SIN Log)
# ================================================
print("\n--- 9. Entrenando modelo XGBoost (SIN Log) ---")

params = {
    'n_estimators': [200],
    'max_depth': [3],
    'learning_rate': [0.05],
    'subsample': [0.8],
}
reg = XGBRegressor(random_state=42, n_jobs=-1, objective='reg:squarederror')
grid = GridSearchCV(
    reg, param_grid=params, scoring="neg_mean_squared_error",
    cv=5, n_jobs=-1, verbose=1, error_score='raise'
)

print("Iniciando GridSearchCV con XGBoost...")
total_combinaciones = np.prod([len(v) for v in params.values()])
print(f"Probando {total_combinaciones} combinaciones...")

try:
    grid.fit(X_train, y_train) # *** Entrenar con y_train (original) ***
    print("\n‚úÖ ¬°GridSearchCV completado!")
    print("Mejor configuraci√≥n encontrada (o usada):", grid.best_params_)
    best_model = grid.best_estimator_

except Exception as e:
    print(f"‚ùå Error durante el GridSearchCV: {e}")
    best_model = None


# ================================================
# 10. Evaluaci√≥n del Modelo (SIN Log)
# ================================================
print("\n--- 10. Evaluando el modelo (SIN Log) ---")

if best_model is None:
    print("‚ùå No se pudo evaluar.")
else:
    try:
        y_pred_train = best_model.predict(X_train)
        y_pred_test  = best_model.predict(X_test)

        y_pred_train[y_pred_train < 0] = 0 # Asegurar no negativos
        y_pred_test[y_pred_test < 0] = 0

        print("\nCalculando m√©tricas contra 'y' original...")
        mae_train = mean_absolute_error(y_train, y_pred_train)
        mae_test  = mean_absolute_error(y_test,  y_pred_test)
        rmse_train = np.sqrt(mean_squared_error(y_train, y_pred_train))
        rmse_test  = np.sqrt(mean_squared_error(y_test,  y_pred_test))
        r2_train = r2_score(y_train, y_pred_train)
        r2_test  = r2_score(y_test,  y_pred_test)

        print("\n--- M√©tricas de Evaluaci√≥n (sobre 'y' original) ---")
        print(f"Train -> MAE: {mae_train:.3f} | RMSE: {rmse_train:.3f} | R¬≤: {r2_train:.3f}")
        print(f"Test  -> MAE: {mae_test:.3f} | RMSE: {rmse_test:.3f} | R¬≤: {r2_test:.3f}")

        if r2_test > 0.1:
            print("\n¬°√âxito! El R¬≤ es positivo y significativo.")
        elif r2_test > 0:
            print("\n¬°Mejora! El R¬≤ es positivo, pero bajo.")
        else:
            print("\nResultado: El R¬≤ sigue siendo negativo o cero.")
            print("El sesgo en 'y' probablemente sigue afectando el rendimiento.")

    except Exception as e:
        print(f"‚ùå Error durante la evaluaci√≥n: {e}")


# ================================================
# 11. Importancia de las variables
# ================================================
print("\n--- 11. Calculando Importancia de Variables ---")

if best_model is None or 'X_train' not in locals():
     print("‚ùå No se puede calcular la importancia.")
else:
    try:
        importances = best_model.feature_importances_
        feature_names = X_train.columns

        imp_df = pd.DataFrame({"Variable": feature_names, "Importancia": importances})
        imp_df = imp_df.sort_values("Importancia", ascending=False)

        print("Generando gr√°fica de importancia (Top 20)...")
        plt.figure(figsize=(10, 8))
        top_20_imp = imp_df.head(20).iloc[::-1]
        plt.barh(top_20_imp["Variable"], top_20_imp["Importancia"])
        plt.title("Importancia de variables (Top 20)")
        plt.xlabel("Importancia (seg√∫n XGBoost)")
        plt.tight_layout()
        plt.show() # Muestra el gr√°fico en una ventana

        print("\nTabla de Importancia (Top 20):")
        print(imp_df.head(20))

        if not imp_df.empty and 'y_lag' in imp_df.iloc[0, 0]:
             print("\nNota: 'y_lag' (calculado sobre 'eggs' original) sigue siendo importante.")
        elif not imp_df.empty:
             print(f"\nNota: La variable m√°s importante ahora es '{imp_df.iloc[0, 0]}'.")


    except Exception as e:
        print(f"‚ùå Error durante el c√°lculo de importancia: {e}")


# ================================================
# 12. Generar Predicciones (Dataset Completo)
# ================================================
print("\n--- 12. Generando Predicciones (Dataset Completo) ---")

if 'best_model' not in locals() or best_model is None:
    print("‚ùå Error: El 'best_model' no fue entrenado. No se pueden generar predicciones.")
elif 'X_spatial' not in locals():
    print("‚ùå Error: La matriz 'X_spatial' no existe. No se pueden generar predicciones.")
elif 'gdf' not in locals():
    print("‚ùå Error: El 'GeoDataFrame' (gdf) no existe.")
else:
    try:
        print("Generando predicciones sobre el dataset completo (X_spatial)...")
        # Predecir sobre todos los datos
        all_predictions = best_model.predict(X_spatial)
        
        all_predictions[all_predictions < 0] = 0
        
        gdf['predicted_eggs'] = all_predictions
        
        print("‚úÖ Predicciones generadas y a√±adidas al GeoDataFrame como 'predicted_eggs'.")
        
        print("\nResumen comparativo (Original vs. Predicho):")
        print(gdf[['eggs', 'predicted_eggs']].describe())

    except Exception as e:
        print(f"‚ùå Error durante la generaci√≥n de predicciones completas: {e}")


# ================================================
# 13. Mapeo de Predicciones (con Shapefile)
# ================================================
print("\n--- 13. Mapeo de Predicciones (con Shapefile) ---")

if 'gdf' not in locals() or 'predicted_eggs' not in gdf.columns:
    print("‚ùå No se puede mapear. El 'gdf' o la columna 'predicted_eggs' no existen.")
else:
    try:
        # --- Carga del Shapefile ---
        print(f"Cargando mapa base desde: {shapefile_filename}")
        mapa_base_gdf = gpd.read_file(shapefile_filename)
        
        # --- Verificaci√≥n y Unificaci√≥n de CRS ---
        print(f"    CRS de puntos (gdf): {gdf.crs}")
        print(f"    CRS de mapa (shapefile): {mapa_base_gdf.crs}")
        
        if gdf.crs != mapa_base_gdf.crs:
            print("    Unificando CRS... (convirtiendo mapa base al CRS de los puntos)")
            mapa_base_gdf = mapa_base_gdf.to_crs(gdf.crs)
        
        print("    CRS unificados.")

        # --- Generaci√≥n del Mapa ---
        print("Generando mapa de calor de las predicciones sobre el mapa base...")
        fig, ax = plt.subplots(figsize=(15, 12)) 
        
        mapa_base_gdf.plot(
            ax=ax,
            color='#E0E0E0',
            edgecolor='white',
            linewidth=0.5
        )
        
        gdf.plot(
            column='predicted_eggs', 
            ax=ax,
            legend=True,
            cmap='YlOrRd',
            markersize=20,
            legend_kwds={'label': "Cantidad de Huevos Predichos", 'shrink': 0.8},
            alpha=0.75
        )
        
        ax.set_title("Mapa de Predicciones de Huevos de Dengue (sobre mapa base)")
        ax.set_xlabel("Longitud (x)")
        ax.set_ylabel("Latitud (y)")
        plt.tight_layout()
        plt.show() # Muestra el gr√°fico en una ventana

    except FileNotFoundError:
        print(f"‚ùå ERROR: No se encontr√≥ el archivo Shapefile '{shapefile_filename}'.")
        print("üëâ Aseg√∫rate de que el archivo y sus componentes (.dbf, .shx, .prj) est√©n en el directorio correcto.")
    except Exception as e:
        print(f"‚ùå Error al generar el mapa de predicciones: {e}")


print("\n--- Fin del Pipeline (con predicci√≥n y mapeo) ---")
